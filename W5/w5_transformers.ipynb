{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from collections import Counter\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/files/100/100-0.txt\"\n",
    "response = requests.get(url)\n",
    "shakespeare_text = response.text\n",
    "\n",
    "shakespeare_text = re.sub(r'\\s+', ' ', shakespeare_text)\n",
    "hamlet_text = shakespeare_text[927066:1105544]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "vocab = Counter(tokenize(hamlet_text)) # Counter: 단어의 빈도수를 셈\n",
    "vocab['<PAD>'] = 0 # 패딩용 단어\n",
    "vocab['<UNK>'] = 1 # OOV(Out-Of-Vocabulary) 단어\n",
    "vocab = {word: i+2 for i, (word, _) in enumerate(vocab.items())} # 각 단어에 고유한 정수 부여(2 이상의 정수부터)\n",
    "\n",
    "def encode(text, vocab):\n",
    "    # 텍스트를 띄어쓰기 기준으로 토큰화하고, 각 토큰을 정수로 변환\n",
    "    return [vocab.get(token, vocab['<UNK>']) for token in tokenize(text)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, vocab, seq_length):\n",
    "        self.sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text) # 문장 단위로 분리\n",
    "        self.vocab = vocab\n",
    "        self.seq_length = seq_length\n",
    "        self.encoded_sentences = [encode(sentence, vocab) for sentence in self.sentences] # 단어를 정수로 변환\n",
    "        self.encoded_sentences = [sent for sent in self.encoded_sentences if len(sent) > 1]  # 빈 문장 제거\n",
    "\n",
    "    def __len__(self):\n",
    "        # 데이터셋의 총 샘플 수: 전체 문장의 개수(encoded_sentences)\n",
    "        return \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # idx번째 샘플을 반환\n",
    "        sentence = self.encoded_sentences[idx]\n",
    "        # 입력 시퀀스: 문장의 처음부터 seq_length-1까지\n",
    "        input_seq = \n",
    "        # 타깃 시퀀스: 문장의 두 번째 단어부터 seq_length까지\n",
    "        target_seq = \n",
    "\n",
    "        # 문장 길이(len(input_seq))가 seq_length보다 작은 경우 패딩 추가\n",
    "        if \n",
    "            # 부족한 만큼 패딩 추가: (seq_length - len(input_seq), len(target_seq))\n",
    "            input_seq.extend()\n",
    "            target_seq.extend()\n",
    "\n",
    "        # input_seq, target_seq를 torch tensor로 변환\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    '''\n",
    "    DataLoader에서 미니배치를 처리하는 함수: 패딩을 적용하여 길이를 맞춰줌\n",
    "    '''\n",
    "    \n",
    "    inputs, targets = zip(*batch) # 미니배치에서 각 샘플의 입력과 타깃을 분리\n",
    "    inputs = pad_sequence(inputs, batch_first=True, padding_value=vocab['<PAD>']) # 입력 시퀀스에 대해 패딩 추가\n",
    "    targets = pad_sequence(targets, batch_first=True, padding_value=vocab['<PAD>']) # 타깃 시퀀스에 대해 패딩 추가\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Q, K, V에 해당하는 가중치 행렬 생성\n",
    "        # nn.Linear로 d_model만큼의 차원의 W_q, W_k, W_v를 만듦\n",
    "        self.query = \n",
    "        self.key = \n",
    "        self.value = \n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # 초기 Q, K, V 입력 차원: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # 1. Linear transformation: Q, K, V에 각각 가중치 행렬을 곱함\n",
    "        # 2. View & Transpose: multi-head attention을 수행하기 위해 텐서 형태 변환\n",
    "            # view: (batch_size, seq_len, d_model) -> (batch_size, seq_len, num_heads, d_k)\n",
    "            # transpose: (batch_size, num_heads, seq_len, d_k) -> (batch_size, num_heads, d_k, seq_len)\n",
    "            # d_k: 각 head가 담당하는 d_model의 차원 크기\n",
    "        q = \n",
    "        k = \n",
    "        v = \n",
    "        \n",
    "        # 3. Scaled Dot Product Attention: 각 head 별로 어텐션 수행\n",
    "        # (batch_size, num_heads, seq_len, d_k) @ (batch_size, num_heads, d_k, seq_len) -> (batch_size, num_heads, seq_len, seq_len)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # 4. Masking\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # 5. Softmax: 각 head 별로 어텐션 스코어(scaled attention score) 계산\n",
    "        attn = F.\n",
    "        \n",
    "        # 6. Weighted Sum: 각 head 별로 V에 대한 어텐션 값(attention value) 계산\n",
    "        # (batch_size, num_heads, seq_len, seq_len) @ (batch_size, num_heads, seq_len, d_k) -> (batch_size, num_heads, seq_len, d_k)\n",
    "        context = torch.\n",
    "        \n",
    "        # 7. Concatenation: 각 head 별 결과를 하나로 합침\n",
    "            # transpose: (batch_size, num_heads, seq_len, d_k) -> (batch_size, seq_len, num_heads, d_k)\n",
    "            # contiguous: 메모리 상에 연속적인 공간에 재배열\n",
    "            # view: (batch_size, seq_len, num_heads, d_k) -> (batch_size, seq_len, num_heads * d_k = d_model)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # 8. Final Linear: multi-head attention의 결과에 대해 최종 가중치 행렬을 곱함\n",
    "        # (batch_size, seq_len, d_model)\n",
    "        output = self.fc(context)\n",
    "        \n",
    "        return output # (batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        # nn.Linear로 d_model 차원을 d_ff 차원으로 변경\n",
    "        self.fc1 = \n",
    "        # nn.Linear로 d_ff 차원을 d_model 차원으로 변경\n",
    "        self.fc2 = \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # fc1 -> ReLU -> dropout -> fc2\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model) # (max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) # (max_len, 1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model/2)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # 짝수 인덱스에는 sin 함수 적용\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # 홀수 인덱스에는 cos 함수 적용\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1) # (max_len, 1, d_model) -> (max_len, d_model, 1)\n",
    "        self.register_buffer('pe', pe) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :] # 입력(x)과 positional encoding(self.pe)를 더함\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        # Multi-Head Attention\n",
    "        self.self_attn = \n",
    "        # Positionwise FeedForward\n",
    "        self.feed_forward = \n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Multi-Head Attention: self-attention\n",
    "        # Q, K, V에 입력 x를 넣고, 마스킹은 mask를 사용\n",
    "        attn_output = \n",
    "        # Layer Normalization & Residual Connection\n",
    "        # 입력 x와 attention 결과를 더하고 Layer Normalization\n",
    "        x = \n",
    "        \n",
    "        # Positionwise FeedForward\n",
    "        ff_output = \n",
    "        # Layer Normalization & Residual Connection\n",
    "        # 이전 단계의 출력과 Positionwise FeedForward 결과를 더하고 Layer Normalization\n",
    "        x = \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodrLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        # Multi-Head Attention (self-attention)\n",
    "        self.self_attn = \n",
    "        # Multi-Head Attention (encoder-decoder attention)\n",
    "        self.cross_attn =\n",
    "        # Positionwise FeedForward\n",
    "        self.feed_forward = \n",
    "        self.layernorm1 = nn.LayerNorm(d_model)\n",
    "        self.layernorm2 = nn.LayerNorm(d_model)\n",
    "        self.layernorm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        # Multi-Head Attention: self-attention\n",
    "        # Q, K, V에 입력 x를 넣고, 마스킹은 tgt_mask를 사용\n",
    "        self_attn_output = \n",
    "        # Layer Normalization & Residual Connection\n",
    "        # 입력 x와 self-attention 결과를 더하고 Layer Normalization\n",
    "        x = \n",
    "        \n",
    "        # Multi-Head Attention: encoder-decoder attention\n",
    "        # Q: decoder layer의 출력 x, K, V: encoder의 출력 enc_output\n",
    "        cross_attn_output = \n",
    "        # Layer Normalization & Residual Connection\n",
    "        # 이전 단계의 출력과 encoder-decoder attention 결과를 더하고 Layer Normalization\n",
    "        x = \n",
    "        \n",
    "        # Positionwise FeedForward\n",
    "        ff_output = \n",
    "        # Layer Normalization & Residual Connection\n",
    "        # 이전 단계의 출력과 Positionwise FeedForward 결과를 더하고 Layer Normalization\n",
    "        x = \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, num_heads=8, d_ff=2048, num_layers=6, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        # Embedding\n",
    "        # encoder, decoder의 입력 차원: src_vocab_size, tgt_vocab_size -> d_model\n",
    "        self.encoder_embedding = \n",
    "        self.decoder_embedding = \n",
    "        \n",
    "        # Positional Encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model)\n",
    "        \n",
    "        # Encoder & Decoder\n",
    "        # num_layers만큼 EncoderLayer, DecoderLayer를 쌓음\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = \n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        # Encoder: Embedding & Positional Encoding\n",
    "        src = \n",
    "        \n",
    "        # Encoder: 입력 문장을 입력받아 인코더 레이어(self.encoder_layers)를 차례대로 거침\n",
    "        for layer in self.encoder_layers:\n",
    "            src = layer(src, src_mask)\n",
    "        \n",
    "        # Decoder: Embedding & Positional Encoding\n",
    "        tgt = \n",
    "        \n",
    "        # Decoder: 출력 문장을 입력받아 디코더 레이어(self.decoder_layers)를 차례대로 거침\n",
    "        for \n",
    "        \n",
    "        # Fully Connected Layer: decoder 출력을 이용해 타겟 단어의 logit을 계산\n",
    "        output = \n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sentence_length(text):\n",
    "    '''\n",
    "    문장의 평균 단어 개수 계산하는 함수\n",
    "    '''\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    total_words = sum(len(tokenize(sentence)) for sentence in sentences)\n",
    "    average_length = total_words / len(sentences)\n",
    "    return average_length\n",
    "\n",
    "average_length = average_sentence_length(hamlet_text)\n",
    "print(average_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "seq_length = 10 # 문장의 길이(단어 개수)\n",
    "batch_size = 512\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Model, Loss, Optimizer 생성\n",
    "model = \n",
    "criterion = \n",
    "optimizer = \n",
    "\n",
    "# TextDataset 생성\n",
    "dataset = \n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# train_loader, test_loader 생성\n",
    "train_loader = \n",
    "test_loader = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model, train_loader, test_loader, criterion, optimizer, epochs, device):\n",
    "    # 모델을 device에 할당\n",
    "    \n",
    "    \n",
    "    # epochs만큼 반복\n",
    "    for \n",
    "        # Training\n",
    "        # model을 train 모드로 설정\n",
    "        \n",
    "        # loss 초기화\n",
    "        \n",
    "        # train_loader를 이용해 학습\n",
    "        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Training]\", leave=False):\n",
    "            # inputs, targets를 device에 할당\n",
    "            \n",
    "            # gradient 초기화\n",
    "            optimizer.zero_grad()\n",
    "            # model에 inputs를 넣어 output을 계산\n",
    "            outputs = \n",
    "            \n",
    "            # loss 계산 및 역전파\n",
    "            loss = criterion(outputs.view(-1, outputs.size(2)), targets.view(-1))\n",
    "            loss.\n",
    "            optimizer.\n",
    "\n",
    "            # loss 누적\n",
    "            train_loss\n",
    "            \n",
    "        # 평균 train loss 계산(epoch 당)\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Testing\n",
    "        # model을 eval 모드로 설정\n",
    "        \n",
    "        # loss 초기화\n",
    "        \n",
    "        # gradient 계산 비활성화\n",
    "        \n",
    "            # test_loader를 이용해 테스트\n",
    "            for inputs, targets in tqdm(test_loader, desc=f\"Epoch {epoch+1}/{epochs} [Testing]\", leave=False):\n",
    "                # inputs, targets를 device에 할당\n",
    "\n",
    "                # model에 inputs를 넣어 output을 계산\n",
    "                outputs = model(inputs, inputs)\n",
    "                # loss 계산 및 누적\n",
    "                loss = criterion(outputs.view(-1, outputs.size(2)), targets.view(-1))\n",
    "                test_loss\n",
    "                \n",
    "        # 평균 test loss 계산(epoch 당)\n",
    "        avg_test_loss = \n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "# Train the model\n",
    "train_and_test(model, train_loader, test_loader, criterion, optimizer, epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function\n",
    "def predict(model, start_sequence, vocab, seq_length, max_length=100):\n",
    "    model.eval()\n",
    "    tokens = tokenize(start_sequence)\n",
    "    generated = tokens\n",
    "    for _ in range(max_length):\n",
    "        input_seq = torch.tensor([vocab.get(token, vocab['<UNK>']) for token in generated[-seq_length:]], dtype=torch.long).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq, input_seq)\n",
    "        \n",
    "        next_token = output[:, -1, :].argmax(dim=-1).item()\n",
    "        \n",
    "        next_word = [word for word, idx in vocab.items() if idx == next_token][0]\n",
    "        generated.append(next_word)\n",
    "        \n",
    "        if next_word == '<PAD>':\n",
    "            break\n",
    "\n",
    "    return ' '.join(generated)\n",
    "\n",
    "input_text = \"to be or not to be that is the question\"\n",
    "predicted_output = predict(model, input_text, vocab, seq_length)\n",
    "print(predicted_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
